{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Search\n",
    "\n",
    "A lot of NLP projects use Twitter as data source. Here we'll see some ways to obtain Tweets for use in your applications. One way to start testing searches for Tweets, is to first use the twitter.com/search UI, and build an API version from its guidance. There is absolutely not complete parity or completeness, but it's enough to get started. \n",
    "\n",
    "We want to search for Tweets referencing Barcelona. First, we run the search on twitter.com/search\n",
    "\n",
    "https://twitter.com/search?q=barcelona\n",
    "\n",
    "Once we have the desired query, there are several tools to obtain Tweets programmatically. The ways explained in this post are based on Python, and some of them are based on the official Twitter RESTful API, and some of them not.\n",
    "\n",
    "The easiest (ans safest) one is the Twitter API. The Twitter API platform offers three tiers of search APIs, but here we'll focus on two of them. The first one is the Twitter Standard which is free and with the restriction that searches against a sampling of recent Tweets published in the past 7 days. To overcome these limits, Twitter offers a Premium API, which has free and paid access to either the last 30 days of Tweets or access to Tweets from as early as 2006. [Twitter Search explanation](https://developer.twitter.com/en/docs/tweets/search/overview).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path, PurePath\n",
    "\n",
    "TWITTER_QUERY = \"barcelona\"\n",
    "DATA_DIR = Path.cwd() / \"data\"\n",
    "\n",
    "Path(DATA_DIR).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter search with Standard API\n",
    "\n",
    "[Twitter Standard API documentation](https://developer.twitter.com/en/docs/tweets/search/overview/standard)\n",
    "\n",
    "### Using Python \n",
    "\n",
    "First step is Oauth2 authentication. To start, a new app must be created (here](https://developer.twitter.com/app/new). Then click on the 'Keys and Access Tokens' page and retrieve the Consumer Key and Consumer Secret. Open the file `secrets.py.example`, save a copy as `secrets.py`, and finally fill the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secrets import APP_KEY, APP_SECRET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to obtain a Bearer Token. To do this, make a `POST` request to the authentication endpoint, and the returned value will be included in subsequent API requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAAAAAAAAAAAAAAAAAAAANL0igAAAAAA6prUdL83hZg2sJEJgzEPMPwYG5g%3DSDALebPMTa2qlNgkfR5vCzNcRFS0dZn2QfuuZ7arDYKt6cT9XM\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import base64\n",
    "\n",
    "key_secret = f'{APP_KEY}:{APP_SECRET}'.encode('ascii')\n",
    "b64_encoded_key = base64.b64encode(key_secret)\n",
    "b64_encoded_key = b64_encoded_key.decode('ascii')\n",
    "\n",
    "base_url = 'https://api.twitter.com/'\n",
    "auth_url = f'{base_url}oauth2/token'\n",
    "\n",
    "auth_headers = {\n",
    "    'Authorization': f'Basic {b64_encoded_key}',\n",
    "    'Content-Type': 'application/x-www-form-urlencoded;charset=UTF-8'\n",
    "}\n",
    "\n",
    "auth_data = {\n",
    "    'grant_type': 'client_credentials'\n",
    "}\n",
    "\n",
    "auth_resp = requests.post(auth_url, headers=auth_headers, data=auth_data)\n",
    "\n",
    "# Check status code okay\n",
    "assert auth_resp.status_code==200\n",
    "\n",
    "# Keys in data response are token_type (bearer) and access_token (your access token)\n",
    "access_token = auth_resp.json()['access_token']\n",
    "print(access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter documentation has extense information about the API use. The reference to make queries can be found at https://developer.twitter.com/en/docs/api-reference-index. \n",
    "\n",
    "Different options for search parameters can be found at https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miles de personas protestan en Barcelona contra los 21 puntos de Torra https://t.co/wlIyYt3bly\n",
      "\n",
      "@chucksantesteve @castell_st El 16 a les 17h a Barcelona: #SEdR, #SantEsteveDelesRoures, @st_esteveroures i‚Ä¶ https://t.co/RnoMFwLGsU\n",
      "\n",
      "RT @GuajeSalvaje: Quienes no han podido ir a Madrid, tambi√©n se han manifestado en Barcelona por la unidad de Espa√±a.\n",
      "Os dejo aqu√≠ el üìΩÔ∏è po‚Ä¶\n",
      "\n",
      "RT @CUPGranollers: Hi ha qui ara vol fer veure que s'escandalitza molt fort davant les demostracions de neofeixisme, quan fa no gaire anava‚Ä¶\n",
      "\n",
      "RT @crpandemonium: Barcelona se rindi√≥ al fascismo en 1939 sin pegar un solo tiro y vitoreando a Franco mientras Madrid resist√≠a durante 3‚Ä¶\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_headers = {\n",
    "    'Authorization': f'Bearer {access_token}'\n",
    "}\n",
    "\n",
    "search_params = {\n",
    "    'q': TWITTER_QUERY,\n",
    "    'result_type': 'recent',\n",
    "    'count': 5\n",
    "}\n",
    "\n",
    "search_url = f'{base_url}1.1/search/tweets.json'\n",
    "search_resp = requests.get(search_url, headers=search_headers, params=search_params)\n",
    "assert search_resp.status_code==200\n",
    "\n",
    "# There is a lot of information that comes with this data, \n",
    "# including search metadata, geolocations, twitter author information etc. \n",
    "# Here only use the 'statuses' objects.\n",
    "tweets = search_resp.json()\n",
    "\n",
    "# Print the tweet text\n",
    "for tweet in tweets['statuses']:\n",
    "    print(tweet['text'] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Python library\n",
    "\n",
    "Another way to use the Twitter API is with third party libraries. In this case we use [Twython](https://twython.readthedocs.io/en/latest/), a pure Python wrapper that supports both normal and streaming Twitter APIs. Those libraries tend to simplify the access to the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twython import Twython\n",
    "\n",
    "twitter = Twython(APP_KEY, APP_SECRET, oauth_version=2)\n",
    "ACCESS_TOKEN = twitter.obtain_access_token()\n",
    "twitter = Twython(APP_KEY, access_token=ACCESS_TOKEN)\n",
    "    \n",
    "tweets = twitter.search(q=TWITTER_QUERY)\n",
    "\n",
    "# In this case, save the tweets in different json files\n",
    "for tweet in tweets['statuses']:\n",
    "    filename = DATA_DIR / (tweet['id_str'] + '.json')\n",
    "    with open(filename, 'w') as fp:\n",
    "        json.dump(tweet, fp, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter search with Premium API\n",
    "\n",
    "If 7 days back is not enough, for a given project, then it's necessary use the \n",
    "[Twitter Premium API](https://developer.twitter.com/en/docs/tweets/search/overview/premium). This API gives the possibility to go back until 2006, or to use [premium operators](https://developer.twitter.com/en/docs/tweets/search/guides/premium-operators) not available with the Standard API.\n",
    "    \n",
    "[SearchTweets](https://github.com/twitterdev/search-tweets-python), a Python wrapper for Twitter Premium and Enterprise Search APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grabbing bearer token from OAUTH\n"
     ]
    }
   ],
   "source": [
    "from searchtweets import gen_rule_payload, load_credentials, collect_results\n",
    "\n",
    "premium_search_args = load_credentials(\"./twitter_keys.yml\",\n",
    "                                       yaml_key=\"search_tweets_premium\",\n",
    "                                       env_overwrite=False)\n",
    "\n",
    "rule = gen_rule_payload(TWITTER_QUERY, from_date=\"2019-02-01\", results_per_call=500)\n",
    "tweets = collect_results(rule, max_results=500, result_stream_args=premium_search_args)\n",
    "filename = DATA_DIR / 'tweets_premium.json'\n",
    "with open(filename, 'w') as fp:\n",
    "    json.dump(tweets, fp, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter scrapers\n",
    "\n",
    "An alternative to the Twitter APIs to download Tweets is scraping the Twitter site directly. There are some useful tools like allowing to scrape a user's followers, following, Tweets and more while evading most API limitations. here we'll use [Twint](https://github.com/twintproject/twint) a Twitter scraping tool written in Python. Twint scraper is based on the Beautifulsoup library to parse the Twitter pages. \n",
    "\n",
    "In this example, we'll execute the same query, and results will be saved in a csv file into the `data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import twint\n",
    "\n",
    "c = twint.Config()\n",
    "c.Search = TWITTER_QUERY\n",
    "c.Store_csv = True\n",
    "c.Output = \"data\"\n",
    "c.Since = \"2019-01-01\"\n",
    "c.Limit = 100\n",
    "c.Hide_output = True\n",
    "twint.run.Search(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more advanced python users, an alternative is to write your own scraper using [chromedriver](https://sites.google.com/a/chromium.org/chromedriver/), and Selenium to open up a browser and visit Twitter's search page. A nice example can be found on this [tutorial from Data4Democracy](https://github.com/Data4Democracy/tutorials/blob/master/Twitter/Twitter_Gettingpast_32K_Limit.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
